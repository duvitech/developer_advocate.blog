#Introduction
Historical PowerTrack (HPT) provides an API wrapper around a process that filters and outputs data from a tweet archive.  Data is gathered in a multi-step “job” fashion where the first step is specifying what time period to collect from, and what filters to apply to the tweet archive.

The size of data returned by a job can be immense, both in the number of activities and in the storage size of the output payload. Jobs can produce millions of tweets requiring large amounts of storage space. In order to help ensure file sizes that are quick to download, data files are generated as a 10-minute time-series.  So each file produced covers a ten-minute period of time. Depending on the data velocity associated with the job’s filters, even these 10-minute files can contain many thousands of tweets.  

Since each hour of the job’s time-period can generate up to 6 files (some 10-minute periods will be ‘silent’ and have no data associated with them), the number of data files generated by a HPT job can be large. For example, a 90-day job can produce up to 12,960 files for high-volume filters where there is activity for each 10-minute period.

To download your HPT data you will need to interact with the HPT API (full documentation available [HERE](http://support.gnip.com/apis/historical_api/)); these files are not available via the dashboard. Files are hosted in the cloud for 15 days, after which the files are deleted. This article provides suggestions for and technical details needed to automate the downloads and process these files.   



