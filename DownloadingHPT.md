#Introduction
Historical PowerTrack (HPT) provides an API wrapper around a process that filters and outputs data from a tweet archive.  Data is gathered in a multi-step “job” fashion where the first step is specifying what time period to collect from, and what filters to apply to the tweet archive.

The size of data returned by a job can be immense, both in the number of activities and in the storage size of the output payload. Jobs can produce millions of tweets requiring large amounts of storage space. In order to help ensure file sizes that are quick to download, data files are generated as a 10-minute time-series.  So each file produced covers a ten-minute period of time. Depending on the data velocity associated with the job’s filters, even these 10-minute files can contain many thousands of tweets.  

Since each hour of the job’s time-period can generate up to 6 files (some 10-minute periods will be ‘silent’ and have no data associated with them), the number of data files generated by a HPT job can be large. For example, a 90-day job can produce up to 12,960 files for high-volume filters where there is activity for each 10-minute period.

To download your HPT data you will need to interact with the HPT API (full documentation available [HERE](http://support.gnip.com/apis/historical_api/)); these files are not available via the dashboard. Files are hosted in the cloud for 15 days, after which the files are deleted. 

This article provides suggestions for and technical details needed to automate the downloads and process these files.   

#List of Download Links

As discussed [HERE](http://support.gnip.com/apis/historical_api/), the HPT API provides a method to check on the status of a job.  When a HPT job is created it is assigned a Universally-Unique ID (UUID), and this UUID is used to check on the status of the job of interest. To get the status of this job you make a GET request to the following HPT end-point, which references your Account name and your UUID of interest:

```
https://historical.gnip.com/accounts/<account_name>/publishers/twitter/historical/track/jobs/<uuid>.json
```

```
{
    "title": "WorldCup test",
    "account": "CustomerName",
    "publisher": "twitter",
    "streamType": "track",
    "format": "activity_streams",
    "fromDate": "201405171800",
    "toDate": "201405171900",
    "requestedBy": "me@there.com",
    "requestedAt": "2014-05-19T22:12:27Z",
    "status": "running",
    "statusMessage": "Job queued and being processed.",
    "jobURL": "https://historical.gnip.com:443/accounts/CustomerName/publishers/twitter/historical/track/jobs/kj4r26m0qx.json",
    "quote": {
        "estimatedActivityCount": 12000,
        "estimatedDurationHours": "1.0",
        "estimatedFileSizeMb": "5.0",
        "expiresAt": "2014-05-26T22:15:58Z"
    },
    "acceptedBy": "me@there.com",
    "acceptedAt": "2014-05-19T22:18:45Z",
    "percentComplete": 95
}
```


```
{
    "title": "WorldCup test",
    "account": "CustomerName",
    "publisher": "twitter",
    "streamType": "track",
    "format": "activity_streams",
    "fromDate": "201405171800",
    "toDate": "201405171900",
    "requestedBy": "me@there.com",
    "requestedAt": "2014-05-19T22:12:27Z",
    "status": "delivered",
    "statusMessage": "Job delivered and available for download.",
    "jobURL": "https://historical.gnip.com:443/accounts/CustomerName/publishers/twitter/historical/track/jobs/kj4r26m0qx.json",
    "quote": {
        "estimatedActivityCount": 12000,
        "estimatedDurationHours": "1.0",
        "estimatedFileSizeMb": "5.0",
        "expiresAt": "2014-05-26T22:15:58Z"
    },
    "acceptedBy": "me@there.com",
    "acceptedAt": "2014-05-19T22:18:45Z",
    "results": {
        "activityCount": 11860,
        "fileCount": 6,
        "fileSizeMb": "5.07",
        "completedAt": "2014-05-19T22:30:13Z",
        "dataURL": "https://historical.gnip.com:443/accounts/CustomerName/publishers/twitter/historical/track/jobs/kj4r26m0qx/results.json",
        "expiresAt": "2014-06-03T22:29:46Z"
    },
    "percentComplete": 100
}
```

When your HPT job is finished, the API will make available a list of download links to access your data.  This list is referred to as the “Data URL”, and has the following format.


```
{
    "urlCount": 6,
    "urlList": [
        "https://s3-us-west-1.amazonaws.com/archive.replay.snapshots/snapshots/twitter/track/activity_streams/CustomerName/2014/05/19/20140517-20140517_kj4r26m0qx/2014/05/17/18/00_activities.json.gz?...",
        "https://s3-us-west-1.amazonaws.com/archive.replay.snapshots/snapshots/twitter/track/activity_streams/CustomerName/2014/05/19/20140517-20140517_kj4r26m0qx/2014/05/17/18/10_activities.json.gz?...",
        "https://s3-us-west-1.amazonaws.com/archive.replay.snapshots/snapshots/twitter/track/activity_streams/CustomerName/2014/05/19/20140517-20140517_kj4r26m0qx/2014/05/17/18/20_activities.json.gz?...",
        "https://s3-us-west-1.amazonaws.com/archive.replay.snapshots/snapshots/twitter/track/activity_streams/CustomerName/2014/05/19/20140517-20140517_kj4r26m0qx/2014/05/17/18/30_activities.json.gz?...",
        "https://s3-us-west-1.amazonaws.com/archive.replay.snapshots/snapshots/twitter/track/activity_streams/CustomerName/2014/05/19/20140517-20140517_kj4r26m0qx/2014/05/17/18/40_activities.json.gz?...",
        "https://s3-us-west-1.amazonaws.com/archive.replay.snapshots/snapshots/twitter/track/activity_streams/CustomerName/2014/05/19/20140517-20140517_kj4r26m0qx/2014/05/17/18/50_activities.json.gz?..."
    ],
    "expiresAt": "2014-06-03T22:29:46Z",
    "totalFileSizeBytes": 5324200
}
```

As mentioned above, this list can contain thousands of files. Therefore the downloading needs to be automated, and there are several ways to do this.

#Automating the Download Process

<Pseudo code>

##Download File with cURL


##Downloading Files with a Bash Script
This bash script is another option for downloading Historical PowerTrack data files. One advantage of using this script is that it has ‘smarts’ when restarting a download cycle. If your download cycle gets interrupted for any reason, the script will inspect what files it has already downloaded and download only the files that are not available locally. The script writes files to a ‘downloads’ folder so it is important to keep files there until all the files have been downloaded.

##Downloading Files Using Ruby Application



#Technical Details

Here are some high-level details that provide some technical background on the Historical PowerTrack (HPT) product and the data files it generates:

* Each HPT job has an Universally Unique ID (UUID) associated with it, and this UUID referenced when making API requests and is used to name the resulting files.

###File Details

* HPT generates a set of compressed data files.
     * UTF-8 character-set.
     * Files are GZip compressed. 
     * HPT generates a 10-minute time-series of files. A file is only generated if the ten-minute period it covers has activity. 
     * All file and tweet metadata timestamps are in UTC.
     * Time periods start and include the ‘top’ unit of time and exclude the next ‘top’ unit of time. For example, the first hour of the day (00:00 - 01:00 UTC) would produce up to 6 files covering these 10-minute time periods:   
     * 00:00:00-00:09:59 UTC
     * 00:10:00-00:19:59 UTC
     * 00:20:00-00:29:59 UTC
     * 00:30:00-00:39:59 UTC
     * 00:40:00-00:49:59 UTC
     * 00:50:00-00:59:59 UTC

* Some planning numbers:
     * 6 files per hour.
     * 144 files per day.
     * 4,320 per 30-day month.
     * 52,560 files per year.

* Data is encoded in JSON.
     * Individual activities are written as ‘atomic’ JSON objects, and are not placed in a JSON array.
     * Each file has a single “info” footer: 
```
{"info":{"message":"Replay Request Completed","sent":"2014-05-15T17:47:27+00:00","activity_count":895}}
```

###File-naming conventions:
* HPT file names are a composite of the following details:
     * Job start date, YYYYMMDD
     * Job end date, YYYYMMDD.
     * Job UUID
     * Starting time of 10-minute period, YYYYMMDDHHMM.
     * A static “activities” string.
     * File extension of “.json.gz” (gzip-compressed JSON files).

<start_date>-<end_date>_<Job_UUID><10-min-starting-time>_activities.json.gz

Given a Job UUID of gv96x96q3a covering a period of 2014-05-16 to 2014-05-20, the first hour of 2014-05-17 would produce the following 6 files:
     20140516-20140520_gv96x96q3a201405170000_activities.json.gz
     20140516-20140520_gv96x96q3a201405170010_activities.json.gz
     20140516-20140520_gv96x96q3a201405170020_activities.json.gz
     20140516-20140520_gv96x96q3a201405170030_activities.json.gz
     20140516-20140520_gv96x96q3a201405170040_activities.json.gz
     20140516-20140520_gv96x96q3a201405170050_activities.json.gz


###Link-naming conventions:
https://s3-us-west-1.amazonaws.com/archive.replay.snapshots/snapshots/twitter/track/activity_streams/AccountName/2014/05/01/20080504-20080524_gv96x96q3a/2008/05/04/02/10_activities.json.gz?AWSAccessKeyId=AKIAJP53EAWYQNQDEFAA&Expires=1401596989&Signature=oMd6deTx6hzREjxDzrpHED7NZa4%3D








