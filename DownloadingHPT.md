##Introduction
[Historical PowerTrack (HPT)](http://support.gnip.com/apis/historical_api/) provides an API wrapper around a process that filters and outputs data from a tweet archive. Data are gathered in a multi-step process where the first step is creating a 'job' which specifies what time period to collect from, and what filters to apply to the tweet archive.

The size of data returned by a job can be immense, both in the number of activities and in the storage size of the output payload. Jobs can produce millions of tweets requiring large amounts of storage space. In order to help ensure file sizes that are quick to download, data files are generated as a 10-minute time-series, with each file covering a ten-minute period of time. Depending on the data velocity associated with the job’s filters, even these 10-minute files can contain many thousands of tweets.  

Since each hour of the job’s time-period can generate up to 6 files (10-minute periods can be ‘silent’ and have no data associated with them), the number of data files generated by a HPT job can be large. For example, a 90-day job can produce up to 12,960 files, one for each 10-minute period of those 90 days. 

The data files that are generated are hosted at [Amazon's Simple Storage Service (S3)](http://aws.amazon.com/s3/), and are available for 15 days. When a job is complete, a list of download links is provided via the HPT API. Given that this list can contain thousands of links, some form of download automation is needed to retreive the data.

Below we discuss how to access your download link list, provide some options for automating the downloads, and provide other technical details that will hopefully be helpful when working with HPT data files.  


##Accessing Download Links

As discussed [HERE](http://support.gnip.com/apis/historical_api/api_reference.html), the HPT API provides a method to check on the status of a job. When a HPT job is created it is assigned a Universally-Unique ID (UUID), and this UUID is used to check on the status of the job of interest. To get the status of this job you make a GET request to the following HPT end-point, which references your Account name and the UUID of interest:

```
https://historical.gnip.com/accounts/<account_name>/publishers/twitter/historical/track/jobs/<uuid>.json
```

Note that if you do not have the UUID you can make the following GET request to receive a list of all the jobs that have been submitted:

```
https://historical.gnip.com/accounts/<account_name>/jobs.json
```

When requesting the status of a specific job, the HPT API will respond with a JSON response body that includes "status" and "percentComplete" fields. 

<View GET status response for a job that is running>

```
{
    "title": "WorldCup test",
    "account": "CustomerName",
    "publisher": "twitter",
    "streamType": "track",
    "format": "activity_streams",
    "fromDate": "201405171800",
    "toDate": "201405171900",
    "requestedBy": "me@there.com",
    "requestedAt": "2014-05-19T22:12:27Z",
    "status": "running",
    "statusMessage": "Job queued and being processed.",
    "jobURL": "https://historical.gnip.com:443/accounts/CustomerName/publishers/twitter/historical/track/jobs/kj4r26m0qx.json",
    "quote": {
        "estimatedActivityCount": 12000,
        "estimatedDurationHours": "1.0",
        "estimatedFileSizeMb": "5.0",
        "expiresAt": "2014-05-26T22:15:58Z"
    },
    "acceptedBy": "me@there.com",
    "acceptedAt": "2014-05-19T22:18:45Z",
    "percentComplete": 95
}
```

When a job has completed, the "status" field will be set to "delivered" and the "percentCompleted" will equal 100. Responses for completed jobs are also updated with a "results" section that indicates the number of activities that were compiled and the number of files that were generated. The "results" section also includes a "dataURL" field which is a separate link that provides the list of download links.  

<View GET status response for a completed job>

```
{
    "title": "WorldCup test",
    "account": "CustomerName",
    "publisher": "twitter",
    "streamType": "track",
    "format": "activity_streams",
    "fromDate": "201405171800",
    "toDate": "201405171900",
    "requestedBy": "me@there.com",
    "requestedAt": "2014-05-19T22:12:27Z",
    "status": "delivered",
    "statusMessage": "Job delivered and available for download.",
    "jobURL": "https://historical.gnip.com:443/accounts/CustomerName/publishers/twitter/historical/track/jobs/kj4r26m0qx.json",
    "quote": {
        "estimatedActivityCount": 12000,
        "estimatedDurationHours": "1.0",
        "estimatedFileSizeMb": "5.0",
        "expiresAt": "2014-05-26T22:15:58Z"
    },
    "acceptedBy": "me@there.com",
    "acceptedAt": "2014-05-19T22:18:45Z",
    "results": {
        "activityCount": 11860,
        "fileCount": 6,
        "fileSizeMb": "5.07",
        "completedAt": "2014-05-19T22:30:13Z",
        "dataURL": "https://historical.gnip.com:443/accounts/CustomerName/publishers/twitter/historical/track/jobs/kj4r26m0qx/results.json",
        "expiresAt": "2014-06-03T22:29:46Z"
    },
    "percentComplete": 100
}
```

To access the download list a GET request is made to the Data URL endpoint. The form of this Data URL is:

```
https://historical.gnip.com:443/accounts/<account_name>/publishers/twitter/historical/track/jobs/<uuid>/results.json
```

The HPT API will respond with a JSON response body that includes a "urlList" field which is a JSON array of all the download links associated with the your job.

<View GET response for a Data URL>

```
{
    "urlCount": 6,
    "urlList": [
        "https://s3-us-west-1.amazonaws.com/archive.replay.snapshots/snapshots/twitter/track/activity_streams/CustomerName/2014/05/19/20140517-20140517_kj4r26m0qx/2014/05/17/18/00_activities.json.gz?...",
        "https://s3-us-west-1.amazonaws.com/archive.replay.snapshots/snapshots/twitter/track/activity_streams/CustomerName/2014/05/19/20140517-20140517_kj4r26m0qx/2014/05/17/18/10_activities.json.gz?...",
        "https://s3-us-west-1.amazonaws.com/archive.replay.snapshots/snapshots/twitter/track/activity_streams/CustomerName/2014/05/19/20140517-20140517_kj4r26m0qx/2014/05/17/18/20_activities.json.gz?...",
        "https://s3-us-west-1.amazonaws.com/archive.replay.snapshots/snapshots/twitter/track/activity_streams/CustomerName/2014/05/19/20140517-20140517_kj4r26m0qx/2014/05/17/18/30_activities.json.gz?...",
        "https://s3-us-west-1.amazonaws.com/archive.replay.snapshots/snapshots/twitter/track/activity_streams/CustomerName/2014/05/19/20140517-20140517_kj4r26m0qx/2014/05/17/18/40_activities.json.gz?...",
        "https://s3-us-west-1.amazonaws.com/archive.replay.snapshots/snapshots/twitter/track/activity_streams/CustomerName/2014/05/19/20140517-20140517_kj4r26m0qx/2014/05/17/18/50_activities.json.gz?..."
    ],
    "expiresAt": "2014-06-03T22:29:46Z",
    "totalFileSizeBytes": 5324200
}
```

Note also that the *download list* (not the actual data!) is also available in a CSV format by replacing the ".json" extension of the Data URL with a ".csv" extension, as in:

```
https://historical.gnip.com:443/accounts/<account_name>/publishers/twitter/historical/track/jobs/<uuid>/results.csv
```

When a GET request is made to the CSV endpoint, a file containing the list of download links is downloaded. Each line  contains a file name and the corresponding Amazon S3 link.     


<View GET response for CSV version of Data URL>
```
'20140517-20140517_kj4r26m0qx_2014_05_17_18_00_activities.json.gz'	'https://s3-us-west-1.amazonaws.com/archive.replay.snapshots/snapshots/twitter/track/activity_streams/jim/2014/05/19/20140517-20140517_kj4r26m0qx/2014/05/17/18/00_activities.json.gz?...'
'20140517-20140517_kj4r26m0qx_2014_05_17_18_10_activities.json.gz'	'https://s3-us-west-1.amazonaws.com/archive.replay.snapshots/snapshots/twitter/track/activity_streams/jim/2014/05/19/20140517-20140517_kj4r26m0qx/2014/05/17/18/10_activities.json.gz?...'
'20140517-20140517_kj4r26m0qx_2014_05_17_18_20_activities.json.gz'	'https://s3-us-west-1.amazonaws.com/archive.replay.snapshots/snapshots/twitter/track/activity_streams/jim/2014/05/19/20140517-20140517_kj4r26m0qx/2014/05/17/18/20_activities.json.gz?...'
'20140517-20140517_kj4r26m0qx_2014_05_17_18_30_activities.json.gz'	'https://s3-us-west-1.amazonaws.com/archive.replay.snapshots/snapshots/twitter/track/activity_streams/jim/2014/05/19/20140517-20140517_kj4r26m0qx/2014/05/17/18/30_activities.json.gz?...'
'20140517-20140517_kj4r26m0qx_2014_05_17_18_40_activities.json.gz'	'https://s3-us-west-1.amazonaws.com/archive.replay.snapshots/snapshots/twitter/track/activity_streams/jim/2014/05/19/20140517-20140517_kj4r26m0qx/2014/05/17/18/40_activities.json.gz?...'
'20140517-20140517_kj4r26m0qx_2014_05_17_18_50_activities.json.gz'	'https://s3-us-west-1.amazonaws.com/archive.replay.snapshots/snapshots/twitter/track/activity_streams/jim/2014/05/19/20140517-20140517_kj4r26m0qx/2014/05/17/18/50_activities.json.gz?...'
```



##Automating Downloads

As mentioned above, this list can contain thousands of files. Therefore the downloading needs to be automated, and there are several ways to do this. 


###Download File with cURL

cURL is a handy command-line utility for making HTTP requests. cURL is so useful you will notice that we provide sample cURL commands on the “API Help” tab of the console.gnip.com dashboard, as well as many examples in our support.gnip.com documentation. Gnip is an API company and cURL is a great tool for exercising our many API-based products.


This command downloads all the results files in parallel into a local directory. It downloads an all_files.csv file (silently), and then downloads each file listed in it, printing out the command it will use as it does so. It uses the name column of all_files.csv as the file name.

When using this command, be sure to use the CSV results file, rather than the JSON results file.

```
curl -sS -u<consoleuser>:<password> https://historical.gnip.com/accounts/<account_name>/publishers/twitter/historical/track/jobs/<job_uuid>/results.csv | xargs -P 8 -t -n2 curl -o
```

Download First File - Example

This command downloads the first results file. Try this first if you are having permissions issues downloading the files from S3:

When using this command, be sure to use the CSV results file, rather than the JSON results file.

```
curl -sS -u<user>:<password> https://historical.gnip.com/accounts/<account_name>/publishers/twitter/historical/track/jobs/<job_uuid>/results.csv | head -1 | xargs -P 8 -t -n2 curl -o
```







###Downloading Files with a Bash Script
This bash script is another option for downloading Historical PowerTrack data files. One advantage of using this script is that it has ‘smarts’ when restarting a download cycle. If your download cycle gets interrupted for any reason, the script will inspect what files it has already downloaded and download only the files that are not available locally. The script writes files to a ‘downloads’ folder so it is important to keep files there until all the files have been downloaded.

###Downloading Files Using Ruby Application


###Developing Custom Script/Application


##Technical Details

Here are some high-level details that provide some technical background on the Historical PowerTrack (HPT) product and the data files it generates:

* Each HPT job has an Universally Unique ID (UUID) associated with it, and this UUID referenced when making API requests and is used to name the resulting files.

###File Details

* HPT generates a set of compressed data files.
     * UTF-8 character-set.
     * Files are GZip compressed. 
     * HPT generates a 10-minute time-series of files. A file is only generated if the ten-minute period it covers has activity. 
     * All file and tweet metadata timestamps are in UTC.
     * Time periods start and include the ‘top’ unit of time and exclude the next ‘top’ unit of time. For example, the first hour of the day (00:00 - 01:00 UTC) would produce up to 6 files covering these 10-minute time periods:   
     * 00:00:00-00:09:59 UTC
     * 00:10:00-00:19:59 UTC
     * 00:20:00-00:29:59 UTC
     * 00:30:00-00:39:59 UTC
     * 00:40:00-00:49:59 UTC
     * 00:50:00-00:59:59 UTC

* Some planning numbers:
     * 6 files per hour.
     * 144 files per day.
     * 4,320 per 30-day month.
     * 52,560 files per year.

* Data is encoded in JSON.
     * Individual activities are written as ‘atomic’ JSON objects, and are not placed in a JSON array.
     * Each file has a single “info” footer: 
```
{"info":{"message":"Replay Request Completed","sent":"2014-05-15T17:47:27+00:00","activity_count":895}}
```

###File-naming conventions:
* HPT file names are a composite of the following details:
     * Job start date, YYYYMMDD
     * Job end date, YYYYMMDD.
     * Job UUID
     * Starting time of 10-minute period, YYYYMMDDHHMM.
     * A static “activities” string.
     * File extension of “.json.gz” (gzip-compressed JSON files).

<start_date>-<end_date>_<Job_UUID><10-min-starting-time>_activities.json.gz

Given a Job UUID of gv96x96q3a covering a period of 2014-05-16 to 2014-05-20, the first hour of 2014-05-17 would produce the following 6 files:
     20140516-20140520_gv96x96q3a201405170000_activities.json.gz
     20140516-20140520_gv96x96q3a201405170010_activities.json.gz
     20140516-20140520_gv96x96q3a201405170020_activities.json.gz
     20140516-20140520_gv96x96q3a201405170030_activities.json.gz
     20140516-20140520_gv96x96q3a201405170040_activities.json.gz
     20140516-20140520_gv96x96q3a201405170050_activities.json.gz




